{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6231760-f277-42bb-8253-3fe5adde1594",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#What is language Model?\n",
    "\n",
    "A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Languag.e Model. Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9832570-8f34-4ed0-8f2b-be0673302a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#Generative AI Feature by Databricks\n",
    "\n",
    "\n",
    "Hugging Face Transformers Integration: \n",
    "\n",
    "- Integrated into Databricks Runtime for ML.\n",
    "- Scales NLP batch apps.\n",
    "- Fine-tunes models for large-language apps.\n",
    "- Preinstalled on Databricks Runtime 10.4 LTS ML+.\n",
    "- Optimized for GPU hardware.\n",
    "\n",
    "LangChain Experimentation:\n",
    "\n",
    "- MLflow flavor in Azure Databricks.\n",
    "- Tracks experiments effectively.\n",
    "- Combines LLMs with external data.\n",
    "- Enhances training context.\n",
    "- Included in Databricks Runtime ML 13.1+.\n",
    "\n",
    "Vector Database:\n",
    "\n",
    "- Facilitates storage and retrieval of vector data.\n",
    "- Enhances compatibility with AI workflows.\n",
    "- Supports efficient querying and analysis of vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38e3f7b-efe8-474f-b947-a31cd46dc7ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a92881d8-b9b5-48d3-aae2-044b480668ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#What is DBRX?\n",
    "\n",
    "\n",
    "DBRX, developed by Databricks, stands as a state-of-the-art open source large language model (LLM) utilizing a fine-grained mixture-of-experts (MoE) architecture. This innovative model surpasses benchmarks set by established models like GPT-3.5 and Gemini 1.0 Pro across multiple domains, including language understanding, programming, and mathematics. Notably, DBRX demonstrates remarkable improvements in both training and inference efficiency, making it a highly valuable resource for enterprises and the broader open community.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/HarshithRL/DBRX_RAG/blob/main/Images_for_notebook/dbrx.png?raw=true\"   width=\"800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c5c094-4104-497e-98f0-14d134fc9bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#What is Tokenzer & Embedding Models?\n",
    "\n",
    "A tokenizer in the context of Hugging Face‚Äôs Transformers library is responsible for preparing input data for machine learning models. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/HarshithRL/DBRX_RAG/blob/main/Images_for_notebook/hugging-face-240.png?raw=true\" width=\"100px\" style=\"float: right\" />\n",
    "\n",
    "What Does a Tokenizer Do?\n",
    "- A tokenizer splits input text (such as sentences or paragraphs) into smaller units called tokens. These tokens are then converted into numerical representations (usually integer IDs) that can be fed into a machine learning model.\n",
    "- For example, consider the sentence: ‚ÄúDon‚Äôt you love ü§ó Transformers? We sure do.‚Äù A tokenizer would split it into individual tokens like: [‚ÄúDon‚Äù, ‚Äú'‚Äù, ‚Äút‚Äù, ‚Äúyou‚Äù, ‚Äúlove‚Äù, ‚Äúü§ó‚Äù, ‚ÄúTransformers‚Äù, ‚Äú?‚Äù, ‚ÄúWe‚Äù, ‚Äúsure‚Äù, ‚Äúdo‚Äù, ‚Äú.‚Äù].\n",
    "These tokens are essential for training and using natural language processing (NLP) models like BERT, GPT, and others.\n",
    "\n",
    "\n",
    "What Does Embedding Models Do?\n",
    "\n",
    "\n",
    "- An embedding model is a critical component of LLMs like GPT, responsible for converting textual inputs into numerical representations known as embeddings. These embeddings encode semantic meaning and contextual information, enabling the LLM to comprehend language and produce coherent responses. In essence, the embedding model acts as a translator, bridging the gap between raw text and the LLM's understanding of language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e163b0c2-3be4-4b91-8a37-799f9646450e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "#DBRX is (MoE) State of art model from Databricks\n",
    "DBRXmodel = ChatDatabricks(\n",
    "    endpoint=\"databricks-dbrx-instruct\", \n",
    "    max_tokens=500 \n",
    "    )\n",
    "DBRXEmbedding = DatabricksEmbeddings(endpoint = \"databricks-bge-large-en\") #databricks Foundational Embedding Model\n",
    "GPTTokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\") # DBRX is trained using GPT tokenizer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_components",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
