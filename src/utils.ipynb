{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ae1293-7b95-46be-8476-0b7e66a0ffc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run /Workspace/Users/harshith.r@diggibyte.com/DBRX_dynamic_Class_RAG/load_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52fe640-6554-404f-a569-9968ed4bc2e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is Retrieval Augmented Generation (RAG) for LLMs?\n",
    "\n",
    "<img src=\"https://github.com/HarshithRL/DBRX_RAG/blob/main/Images_for_notebook/RAG.png?raw=true\" width=\"700px\" style=\"float: right\" />\n",
    "\n",
    "RAG, or Retrieval-Augmented Generation, stands out as a potent GenAI technique. It's a fusion of retrieval-based and generation-based models, amplifying both accuracy and adaptability. By feeding custom data into the model, RAG refines its ability to generate precise responses without needing extensive retraining.\n",
    "\n",
    "This approach significantly reduces errors like hallucination, where the model generates irrelevant or inaccurate content. It's a game-changer for applications like chatbots and Q&A systems operating in fields where staying updated with specific information is vital.\n",
    "\n",
    "In simpler terms, RAG ensures that the responses it generates closely match the provided context. This not only boosts performance but also maintains consistency across interactions. And when it comes to response length, RAG can adapt to match the context while delivering varied outputs.\n",
    "\n",
    "### Vector Store & Vector Search\n",
    "\n",
    "To be able to provide additional context to our LLM, we need to search for documents/articles where the answer to our user question might be.\n",
    "To do so,  a common solution is to deploy a vector database. This involves the creation of document embeddings, vectors of fixed size representing your document.<br/>\n",
    "The vectors will then be used to perform real-time similarity search during inference.\n",
    "\n",
    "### Implementing RAG with Databricks AI Foundation models\n",
    "\n",
    "In this demo, we will show you how to build and deploy your custom chatbot, answering questions on any custom or private information.\n",
    "\n",
    "As an example, we will specialize this chatbot to answer questions over Databricks, feeding databricks.com documentation articles to the model for accurate answers.\n",
    "\n",
    "Here is the flow we will implement:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6e04dd-cac4-477f-bed4-9a8b52711a50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-managed-flow-0.png?raw=true\" style=\"margin-left: 10px\"  width=\"1100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e157731-df1e-459e-8abf-2a538b546244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.core.display import Markdown\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f6c0a7-4587-423a-8d12-d5f706efca22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"PROMPT_TEMPLATE\" : \"\"\"\n",
    "        Your a Databricks Assistant ,\n",
    "        Expert In Artificial Intelligence , Generative AI in particular\n",
    "\n",
    "        Answer the Question as detailed as possible from the provided context, make sure to provide all the details in a structured Way, if the answer is not in \n",
    "        provided context just say , 'answer is not available in the context' , don't provide the wrong answer &  if the answer is yes or no condition and the content is not in Provided context say No or else say ,\n",
    "        \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5c369d-1eac-4318-9262-fc8389eff21a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RAGmodule:\n",
    "    \"\"\"\n",
    "    Class for utilizing Retrieval-Augmented Generation (RAG) model in a chatbot application.\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_path: str, tokenizer_model, embeddings_model, chat_model) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the RAGmodule.\n",
    "\n",
    "        Parameters:\n",
    "            pdf_path (str): Path to the PDF document.\n",
    "            tokenizer_model: Model for tokenization.\n",
    "            embeddings_model: Model for generating embeddings.\n",
    "            chat_model: Model for chat interactions.\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.parse_text(pdf_path)  # Parse text from PDF\n",
    "        tokens = self.tokenize(text, tokenizer_model)  # Tokenize text\n",
    "        self.vector_index = self.create_vector_index(tokens, embeddings_model)  # Create vector index\n",
    "        self.chat_model = chat_model  # Assign chat model\n",
    "    \n",
    "    def parse_text(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Parse text from a PDF document.\n",
    "\n",
    "        Parameters:\n",
    "            pdf_path (str): Path to the PDF document.\n",
    "\n",
    "        Returns:\n",
    "            str: Parsed text from the PDF.\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        for page in PdfReader(pdf_path).pages:\n",
    "            text += page.extract_text()  # Extract text from each page\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text, tokenizer_model) -> list:\n",
    "        \"\"\"\n",
    "        Tokenize the text using a tokenizer model.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): Input text to be tokenized.\n",
    "            tokenizer_model: Model for tokenization.\n",
    "\n",
    "        Returns:\n",
    "            list: List of tokenized chunks.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer_model, \n",
    "            chunk_size=500, \n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        text = str(text)\n",
    "        chunks = text_splitter.create_documents([text])  # Create tokenized chunks\n",
    "        return chunks\n",
    "\n",
    "    def create_vector_index(self, chunks, embeddings_model):\n",
    "        \"\"\"\n",
    "        Create vector index from chunks using an embeddings model.\n",
    "\n",
    "        Parameters:\n",
    "            chunks: List of tokenized chunks.\n",
    "            embeddings_model: Model for generating embeddings.\n",
    "        \"\"\"\n",
    "        return FAISS.from_documents(chunks, embedding = embeddings_model)  # Create vector index\n",
    "\n",
    "    def get_conve_chain(self):\n",
    "        \"\"\"\n",
    "        Get conversation chain for chat interactions.\n",
    "        \"\"\"\n",
    "        prompt_suffic = \"Context:\\n {context}?\\n\\nQuestion: \\n{question}\\n\\n\\nAnswer:\"\n",
    "        prompt_template_final = config['PROMPT_TEMPLATE'] + prompt_suffic\n",
    "        prompt = PromptTemplate(\n",
    "            template = prompt_template_final,\n",
    "            input_variables = [\"context\", \"question\"],\n",
    "            callbacks = [StrOutputParser]\n",
    "        )\n",
    "        return load_qa_chain(self.chat_model, chain_type = \"stuff\", prompt=prompt)  # Load conversation chain\n",
    "\n",
    "    def query(self, user_query):\n",
    "        \"\"\"\n",
    "        Query the chat model with user input.\n",
    "\n",
    "        Parameters:\n",
    "            user_query (str): User's query.\n",
    "\n",
    "        Returns:\n",
    "            Markdown: Response in Markdown format.\n",
    "        \"\"\"\n",
    "        docs = self.vector_index.similarity_search(user_query)  # Search for similar documents\n",
    "        chain = self.get_conve_chain()  # Get conversation chain\n",
    "        response = chain({'input_documents': docs, \"question\": user_query}, return_only_outputs=True)  # Get response\n",
    "        return Markdown(response['output_text'])  # Return response in Markdown format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dae7532-78ea-4e56-b222-5f6adb4d43fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
